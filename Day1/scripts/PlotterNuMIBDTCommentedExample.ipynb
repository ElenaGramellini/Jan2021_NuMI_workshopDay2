{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with some useful jupyter magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My big fat greek import list:\n",
    "# Now we import a number of python packages that will be useful later on. I'm going to comment on some notable ones\n",
    "import sys   # sys helps you manipulate environmental variable (e.g. $PWD) from you python environment\n",
    "import localSettings as ls  # localSettings.py is a python script you should have in the PELEE folder: check if you do\n",
    "import math\n",
    "\n",
    "# What time is it? It's time to profile your code! datetime helps you estimating how long part of your code take to run \n",
    "# so you know what's worth spending time optimizing (and what isn't)\n",
    "from datetime import datetime\n",
    "import time\n",
    "now = datetime.now()\n",
    "date_time = now.strftime(\"%m%d%Y\")\n",
    "print(\"date and time:\",date_time)\n",
    "\n",
    "# plotter.py is where the plotter class is define (a lot of the plotting matplotlib magic happens there)\n",
    "import plotter\n",
    "import importlib\n",
    "importlib.reload(plotter)\n",
    "\n",
    "#uproot is a reader and a writer of the ROOT file format using only Python and Numpy. \n",
    "#Unlike the standard C++ ROOT implementation, Uproot is only an I/O library, \n",
    "#primarily intended to stream data into machine learning libraries in Python. \n",
    "import uproot\n",
    "\n",
    "# the next 3 are commonly used python packages for (fast) computations\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib\n",
    "\n",
    "# These specific packages allow you to do rotations in python within a reasonable amount of time\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from numpy import arccos, array\n",
    "from numpy.linalg import norm\n",
    "\n",
    "#pickle and xgboost is what you need if you are going to use BDTs in here\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import nue_booster \n",
    "importlib.reload(nue_booster)\n",
    "import awkward\n",
    "\n",
    "params = {\n",
    "    'axes.labelsize': 'x-large',\n",
    "    'axes.titlesize': 'x-large',\n",
    "    'xtick.labelsize': 'x-large',\n",
    "    'ytick.labelsize': 'x-large'\n",
    "}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NuMIGeoWeights\n",
    "import importlib\n",
    "importlib.reload(NuMIGeoWeights)\n",
    "numiBeamlineGeoWeights = NuMIGeoWeights.NuMIGeoWeights()\n",
    "#print(numiBeamlineGeoWeights.calculateGeoWeight(14,0.01,5.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is all about translating the BNB reference frame to the NuMI on axis reference frame\n",
    "# You can skip it if you're not planning on using angle variables\n",
    "\n",
    "def theta(v, w): return arccos(v.dot(w)/(norm(v)*norm(w)))* 180 / 3.1415926\n",
    "# Note: returns angle in radians\n",
    "def GetNuMIAngle( px,  py,  pz):\n",
    "    r = R.from_matrix([[0.92103853804025681562   , 0.022713504803924120662, 0.38880857519374290021  ],\n",
    "                       [4.6254001262154668408e-05, 0.99829162468141474651 , -0.058427989452906302359],\n",
    "                       [-0.38947144863934973769  , 0.053832413938664107345, 0.91946400794392302291  ]])  \n",
    "    r = r.inv()\n",
    "    beamCoords = r.apply([px,  py,  pz])\n",
    "    beamDir    = [0,0,1]\n",
    "    #print(theta(beamCoords,beamDir))\n",
    "    return theta(beamCoords,beamDir)\n",
    "\n",
    "def GetNuMIAnglePol( phiBNB,  thetaBNB):\n",
    "    px = math.sin(thetaBNB)*math.cos(phiBNB)\n",
    "    py = math.sin(thetaBNB)*math.sin(phiBNB)\n",
    "    pz = math.cos(thetaBNB)\n",
    "    return GetNuMIAngle( px,  py,  pz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 517,
     "status": "ok",
     "timestamp": 1560556807118,
     "user": {
      "displayName": "Stefano Roberto Soleti",
      "photoUrl": "https://lh4.googleusercontent.com/-hfLpspJu4Q0/AAAAAAAAAAI/AAAAAAAABmA/2kE4rtj8paU/s64/photo.jpg",
      "userId": "10372352518008961760"
     },
     "user_tz": 240
    },
    "id": "6qsD0G-yYJ9K",
    "outputId": "5d52a3ec-50be-44fc-da44-3c0593e98bc6"
   },
   "outputs": [],
   "source": [
    "#The following command loads the path to where the code and root file live\n",
    "print(ls.main_path)\n",
    "main_path = ls.main_path\n",
    "sys.path.append(main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define some basic parameters for this analysis which will be useful later\n",
    "# USING BDT?\n",
    "USEBDT = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwoCIaigYJ9N"
   },
   "outputs": [],
   "source": [
    "# Ntuples and Tree loading time! With full Run 1, this may take a while (~30 min), but we're playing with toy ntuples here\n",
    "# Name of the TTree in the PeLEE Ntuples\n",
    "tree = \"NeutrinoSelectionFilter\"\n",
    "\n",
    "# Name of root files for the various samples \n",
    "# On Beam DATA Sample\n",
    "NUMI = 'neutrinoselection_filt_run1_beamon_beamgood'\n",
    "# Off Beam DATA Sample\n",
    "EXT  = 'neutrinoselection_filt_run1_beamoff'   \n",
    "# Full MC Sample (All Neutrino Types distribuited according to the Flux CV)\n",
    "NU   = 'prodgenie_numi_uboone_overlay_fhc_mcc9_run1_v28_all_snapshot'\n",
    "# Nue+NueBar CC Only MC Sample (neutrinos are generated only within Active Volume)\n",
    "NUE  = 'prodgenie_numi_nue_overlay_mcc9_v08_00_00_48_CV_reco2_run1_reco2'\n",
    "# MC Dirt Sample (neutrinos are generated only outside Active Volume)\n",
    "DRT  = 'neutrinoselection_filt_run1_dirt_overlay'\n",
    "\n",
    "    \n",
    "\n",
    "# Actually fetch the data sets \n",
    "mc   = uproot.open(ls.ntuple_path+NU  +\".root\")[ls.fold][tree]\n",
    "nue  = uproot.open(ls.ntuple_path+NUE +\".root\")[ls.fold][tree]\n",
    "data = uproot.open(ls.ntuple_path+NUMI+\".root\")[ls.fold][tree]\n",
    "ext  = uproot.open(ls.ntuple_path+EXT +\".root\")[ls.fold][tree]\n",
    "dirt = uproot.open(ls.ntuple_path+DRT +\".root\")[ls.fold][tree]\n",
    "# Define a list that contains the data sets\n",
    "uproot_v = [mc,nue,ext,data,dirt]\n",
    "\n",
    "# Define the variables you are going to load (less variables = less time)\n",
    "# Some of them are common to all samples, others are present only in MC (like true variables)\n",
    "# These are examples of variables common to all samples (there's more if you want!)\n",
    "variables = [\"run\", \"sub\", \"evt\", \"nu_pdg\",\"shr_tkfit_nhits_Y\", \"shr_tkfit_nhits_U\", \"shr_tkfit_nhits_V\", \"shr_tkfit_dedx_Y\", \n",
    "             \"shr_tkfit_dedx_U\", \"shr_tkfit_dedx_V\", \n",
    "             \"shr_tkfit_2cm_nhits_Y\", \"shr_tkfit_2cm_nhits_U\", \"shr_tkfit_2cm_nhits_V\", \n",
    "             \"shr_tkfit_2cm_dedx_Y\", \"shr_tkfit_2cm_dedx_U\", \"shr_tkfit_2cm_dedx_V\",             \n",
    "             \"shr_energy_tot_cali\", \"trk_energy_tot\", \"shr_energy\", \"shr_theta\", \"trk_theta_v\", \"trk_phi_v\", \n",
    "             \"trk_score_v\", \"trk_llr_pid_score_v\", \"topological_score\",\"trk_energy_proton_v\", \"trk_calo_energy_y_v\", \"trk_id\", \n",
    "             \"n_tracks_contained\", \"shrsubclusters0\", \"shrsubclusters1\", \"shrsubclusters2\", \n",
    "             \"shr_tkfit_npointsvalid\", \"shr_tkfit_npoints\", \"secondshower_Y_dir\", \"shrclusdir2\", \"trk_theta\", \n",
    "             \"trk_sce_start_x_v\", \"trk_sce_start_y_v\", \"trk_sce_start_z_v\", \n",
    "             \"trk_sce_end_x_v\", \"trk_sce_end_y_v\", \"trk_sce_end_z_v\", \n",
    "             \"shrmoliereavg\", \"secondshower_Y_dot\", \"true_e_visible\",\n",
    "             \"pfnhits\", \"pfnunhits\", \"flash_time\", \"category\", \"shr_score\", \"tksh_distance\", \"tksh_angle\",\n",
    "             \"trkshrhitdist2\", \"hits_ratio\", \"secondshower_Y_nhit\", \"secondshower_Y_vtxdist\", \"CosmicIPAll3D\", \n",
    "             \"CosmicDirAll3D\", \"nslice\", \"selected\", \"_opfilter_pe_beam\", \"_opfilter_pe_veto\", \"n_showers_contained\", \n",
    "             \"reco_nu_vtx_sce_x\", \"reco_nu_vtx_sce_y\", \"reco_nu_vtx_sce_z\", \n",
    "             \"pt\", \"shr_phi\", \"shr_tkfit_gap10_nhits_Y\",\"shr_tkfit_gap10_nhits_U\",\"shr_tkfit_gap10_nhits_V\",\n",
    "             \"shr_tkfit_gap10_dedx_Y\", \"shr_tkfit_gap10_dedx_U\", \"shr_tkfit_gap10_dedx_V\",\n",
    "             \"slnunhits\",\"slnhits\",\"pi0_e\",\"ccnc\",\"shr_pz\",\"shr_py\",\"shr_px\",\"nu_e\"]\n",
    "\n",
    "# These are examples of variables present in MC samples only\n",
    "varMC = [\"true_pt\",\"true_pt_visible\",\"true_p\",\"true_p_visible\",\"true_nu_vtx_t\",\"true_nu_vtx_x\",\"true_nu_vtx_y\",\"true_nu_vtx_z\",\"true_nu_vtx_sce_x\",\"true_nu_vtx_sce_y\",\"true_nu_vtx_sce_z\\\n",
    "\",\"true_nu_px\",\"true_nu_py\",\"true_nu_pz\"]\n",
    "\n",
    "\n",
    "#make elements in the list unique (sometimes you repeat the variable name in the list, this takes care of that)\n",
    "variables = list(set(variables))\n",
    "\n",
    "# These weights are needed to correctly calculate the CV for the MC\n",
    "WEIGHTS     = [\"weightSpline\",\"weightTune\",\"weightSplineTimesTune\",\"ppfx_cv\"]\n",
    "# These weights are needed to calculate the Flux, GENIE and Geant4 Systematics. \n",
    "# For NuMI weightsFlux should always be 1 (cause that's relevant for BNB). Exercise: check if this is true in the next cell\n",
    "SYSTEMATICS = ['weightsFlux','weightsGenie','weightsReint'] \n",
    "PPFX        = ['weightsPPFX'] \n",
    "\n",
    "\n",
    "\n",
    "MCFVARS    = [\"mcf_nu_e\",\"mcf_lep_e\",\"mcf_actvol\",\"mcf_nmm\",\"mcf_nmp\",\"mcf_nem\",\"mcf_nep\",\"mcf_np0\",\"mcf_npp\",\n",
    "              \"mcf_npm\",\"mcf_mcshr_elec_etot\",\"mcf_pass_ccpi0\",\"mcf_pass_ncpi0\",\n",
    "              \"mcf_pass_ccnopi\",\"mcf_pass_ncnopi\",\"mcf_pass_cccpi\",\"mcf_pass_nccpi\"]\n",
    "\n",
    "\n",
    "# Load the variables into your dataframes\n",
    "data = data.pandas.df(variables, flatten=False)\n",
    "ext  = ext.pandas .df(variables, flatten=False)    \n",
    "nue  = nue.pandas .df(variables + varMC + WEIGHTS + SYSTEMATICS + MCFVARS + PPFX, flatten=False)\n",
    "mc   = mc.pandas  .df(variables + varMC + WEIGHTS + SYSTEMATICS + MCFVARS + PPFX, flatten=False)\n",
    "# A few notes on the dirt sample: \n",
    "# 1) we are not likely to need the MCFVARS, so we don't bother loading them\n",
    "# 2) the PPFX weights are currently not present in the Ntuples. It is likely that the dirt has a very small impact\n",
    "#    on your analysis and it's a pain to re-generate. If the dirt becomes a problem, we'll see how to handle it\n",
    "dirt = dirt.pandas.df(variables + varMC + WEIGHTS + SYSTEMATICS, flatten=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where you write some code!!!\n",
    "# Check if the variable weightsFlux is = 1 for all the mc and nue events... GO!\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's take care of MC weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the next cell, we are going to play with the value of the event weights. \n",
    "# Let's store the original ones in some \"safe variable\" in this cell first.\n",
    "# So if we screw up, we don't have to re-load the humongous TTree: we'll have the original weights safely stored here\n",
    "df_v = [mc,nue,dirt]\n",
    "for i,df in enumerate(df_v):\n",
    "    df[ 'currentweightsafe' ] = df['weightSplineTimesTune']\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the weights that we need to apply to the individual MC events \n",
    "# to become our best model of the NuMI data. \n",
    "# This cell calculates the GENIE Tune * PPFX correction to the MC weights and \n",
    "# cleans up extremely high (or low) weight scores (may be due to computational errors).\n",
    "# The clean up is not the best practice (one should dig and understand why the weight are big/negative): \n",
    "# it's a cheat until we figure out the next best thing. Life is not perfect and your analysis won't be either. Please enojy LArbys\n",
    "\n",
    "# We need to do this only for the MC samples (all data events weight = 1)\n",
    "df_v = [mc,nue,dirt]\n",
    "\n",
    "for i,df in enumerate(df_v):\n",
    "    # Create a weight of 1 (or \"no Weight\") for debugging purposes\n",
    "    df[ 'noW' ] = 1\n",
    "    # This portion cleans up the basic weights\n",
    "    df.loc[ df['weightTune'] <= 0, 'weightTune' ] = 1.\n",
    "    df.loc[ df['weightTune'] == np.inf, 'weightTune' ] = 1.\n",
    "    df.loc[ df['weightTune'] > 100, 'weightTune' ] = 1.\n",
    "    df.loc[ np.isnan(df['weightTune']) == True, 'weightTune' ] = 1.   \n",
    "    # This portion cleans up the weights after the GENIE tune is applied\n",
    "    df.loc[ df['weightSplineTimesTune'] <= 0, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ df['weightSplineTimesTune'] == np.inf, 'weightSplineTimesTune' ] = 1.\n",
    "    df.loc[ df['weightSplineTimesTune'] > 100, 'weightSplineTimesTune' ] = 1. #originally 100\n",
    "    df.loc[ np.isnan(df['weightSplineTimesTune']) == True, 'weightSplineTimesTune' ] = 1. \n",
    "\n",
    "    # weightSplineTimesTuneTimesPPFX is our final weight: this is NuMI, so the final weight of the MC events needs \n",
    "    # to take into account the NuMI Flux re-weight called ppfx_cv to simulate NuMI events properly. \n",
    "    df[ 'weightSplineTimesTuneTimesPPFX' ] = df[ 'weightSplineTimesTune' ]*df['ppfx_cv']\n",
    "    \n",
    "    # We also need to fix some cray cray weights in the GENIE systematics. \n",
    "    # This is a bit of a pain in the butt cause we have 600 universe (= 600 weights) for each event\n",
    "    for ievt in range(df.shape[0]):\n",
    "        reweightCondition = ((df['weightsGenie'].iloc[ievt] > 3000) | (df['weightsGenie'].iloc[ievt] < 0)  | \n",
    "                             (df['weightsGenie'].iloc[ievt] == np.inf))\n",
    "        df['weightsGenie'].iloc[ievt][ reweightCondition ] = 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some analyses might need to scale the pi0 portion of the MC to match the Data. \n",
    "# Again, in the next cell we'll be messing around with weights, so let's same an original copy here... \n",
    "# so we only have to re-run the next cell if we want to change the pi0 weighting scheme\n",
    "df_v = [mc,nue,dirt]\n",
    "for i,df in enumerate(df_v):\n",
    "    df[ 'currentweight' ]     = df['weightSplineTimesTuneTimesPPFX']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some quick clean up since some strange events get in there\n",
    "mc   = mc.query('nu_e > 0.')\n",
    "nue  = nue.query('nu_e > 0.')\n",
    "dirt = dirt.query('nu_e > 0.')\n",
    "\n",
    "# Calculate the true neutrino direction off of the NuMI beam\n",
    "dfmc_v = [mc,nue,dirt]\n",
    "for i,df in enumerate(dfmc_v):\n",
    "    df['numi_nu_true_Angle'] = df.apply( lambda x: GetNuMIAngle(x['true_nu_px'],x['true_nu_py'],x['true_nu_pz']) , axis=1)\n",
    "    df['weightsNuMIGeo']     = df.apply( lambda x: numiBeamlineGeoWeights.calculateGeoWeight(x['nu_pdg'],x['nu_e'],x['numi_nu_true_Angle']) , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This piece of code allows you to do it in three ways (flat, energy dependent, no weight at all)\n",
    "scaleOption = 2\n",
    "#removeGenieTune = False\n",
    "for i,df in enumerate(df_v):\n",
    "    if scaleOption == 1:\n",
    "        df['weightSplineTimesTune'] = df[ 'currentweight' ] \n",
    "        df.loc[ df['npi0'] > 0, 'weightSplineTimesTuneTimesPPFX' ] =  (df['weightSplineTimesTuneTimesPPFX'] * 0.759)\n",
    "    elif scaleOption == 2:\n",
    "        df['weightSplineTimesTune'] = df[ 'currentweight' ] \n",
    "        pi0emax = 0.6\n",
    "        df.loc[ (df['pi0_e'] > 0.1) & (df['pi0_e'] < pi0emax) , 'weightSplineTimesTuneTimesPPFX'] = df['weightSplineTimesTuneTimesPPFX']*(1.-0.4*df['pi0_e'])\n",
    "        df.loc[ (df['pi0_e'] > 0.1) & (df['pi0_e'] >= pi0emax), 'weightSplineTimesTuneTimesPPFX'] = df['weightSplineTimesTuneTimesPPFX']*(1.-0.4*pi0emax)\n",
    "    elif scaleOption == 0:\n",
    "        df['weightSplineTimesTuneTimesPPFX'] = df[ 'currentweight' ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create a few more variables starting from the ones stored in the root tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell might also take a while, depending how your write the pandas operations. So, let's time it! \n",
    "# (more at the end of the cell)\n",
    "s = time.time()\n",
    "# We need to do it for all samples!\n",
    "df_v = [mc,nue,ext,data,dirt]\n",
    "\n",
    "for i,df in enumerate(df_v):\n",
    "    up = uproot_v[i]\n",
    "    # E.g. 0 Create simple variable through simple pandas operations on dataframes\n",
    "    df['subcluster']      = df['shrsubclusters0'] + df['shrsubclusters1'] + df['shrsubclusters2']\n",
    "    df['trkfit']          = df['shr_tkfit_npointsvalid'] / df['shr_tkfit_npoints']\n",
    "    df['anglediff_Y']     = np.abs(df['secondshower_Y_dir']-df['shrclusdir2'])\n",
    "    df['trkthetacos']     = np.cos(df['trk_theta'])\n",
    "    \n",
    "    # E.g. 1 store angles for the leading proton in the BNB coordinate system\n",
    "    trk_theta_proton_v    = up.array('trk_theta_v')\n",
    "    trk_phi_proton_v      = up.array('trk_phi_v')\n",
    "    trk_id                = up.array('trk_id')-1 \n",
    "    trk_theta_proton_sel  = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_theta_proton_v,trk_id)])\n",
    "    trk_phi_proton_sel    = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_phi_proton_v,trk_id)])\n",
    "    df['protontheta']     = trk_theta_proton_sel\n",
    "    df['protonphi']       = trk_phi_proton_sel\n",
    "    # E.g. 1.5: store calorimetry vars for the leading proton \n",
    "    trk_score_v         = up.array('trk_score_v')    \n",
    "    trk_llr_pid_v       = up.array('trk_llr_pid_score_v')\n",
    "    trk_energy_proton_v = up.array('trk_energy_proton_v')\n",
    "    trk_calo_energy_y_v = up.array('trk_calo_energy_y_v')\n",
    "    trk_llr_pid_v_sel     = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_llr_pid_v,trk_id)])\n",
    "    trk_energy_proton_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_energy_proton_v,trk_id)])\n",
    "    trk_calo_energy_y_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_calo_energy_y_v,trk_id)])\n",
    "    df['trkpid']          = trk_llr_pid_v_sel\n",
    "    df['protonenergy']    = trk_energy_proton_sel\n",
    "    df['trackcaloenergy'] = trk_calo_energy_y_sel\n",
    "\n",
    "    \n",
    "    # E.g. 2 count Number of proton/MIP candidates starting from the track (reconstructed PID)\n",
    "    c                   = up.array('n_tracks_contained')\n",
    "    # Dummy variables\n",
    "    puppa_obj = []\n",
    "    puppa_trk = []\n",
    "    puppa_shw = []\n",
    "    puppa_prt = [] \n",
    "    puppa_mu  = []\n",
    "    \n",
    "    for pidv, p in zip(trk_llr_pid_v,trk_score_v):\n",
    "        N_obj         = 0\n",
    "        N_recoTrks    = 0\n",
    "        N_recoShowers = 0\n",
    "        N_recoProtons = 0\n",
    "        N_recoMuons   = 0\n",
    "        if len(pidv) > 0:\n",
    "            passPIDCut = (pidv <= 0)\n",
    "            passTrkCut = (p    >= 0)\n",
    "            protonCut  = (passPIDCut & passTrkCut) \n",
    "            muonCut    = (np.logical_not(passPIDCut) & passTrkCut) \n",
    "            N_obj         = len(passTrkCut)\n",
    "            N_recoTrks    = np.count_nonzero(passTrkCut)\n",
    "            N_recoShowers = np.count_nonzero(np.logical_not(passTrkCut))\n",
    "            N_recoProtons = np.count_nonzero(protonCut)\n",
    "            N_recoMuons   = np.count_nonzero(muonCut)\n",
    "        puppa_obj.append(N_obj)\n",
    "        puppa_trk.append(N_recoTrks)\n",
    "        puppa_shw.append(N_recoShowers)    \n",
    "        puppa_prt.append(N_recoProtons)\n",
    "        puppa_mu .append(N_recoMuons)\n",
    "\n",
    "    df['N_obj']           = puppa_obj\n",
    "    df['N_recoTrks']      = puppa_trk\n",
    "    df['N_recoShowers']   = puppa_shw\n",
    "    df['N_recoProtons']   = puppa_prt\n",
    "    df['N_recoMuons']     = puppa_mu\n",
    "    \n",
    "\n",
    "\n",
    "    # E.g. 3 calculate angles wrt NuMI reference framy by using the function we defined earlied\n",
    "    df['shr_numi_Angle']    = df.apply( lambda x: GetNuMIAngle   (x['shr_px']   ,x['shr_py'],x['shr_pz']) , axis=1)\n",
    "    df['proton_numi_Angle'] = df.apply( lambda x: GetNuMIAnglePol(x['protonphi'],x['protontheta']) , axis=1)\n",
    "\n",
    "# How long is this taking??? Print it!\n",
    "s2 = time.time()\n",
    "print (s-s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other example variables you can construct \n",
    "for i,df in enumerate(df_v):\n",
    "    up = uproot_v[i]\n",
    "    #trk_score_v = up.array('trk_score_v')\n",
    "    trk_sce_start_x_v = up.array('trk_sce_start_x_v')\n",
    "    trk_sce_start_y_v = up.array('trk_sce_start_y_v')\n",
    "    trk_sce_start_z_v = up.array('trk_sce_start_z_v')\n",
    "    trk_sce_end_x_v = up.array('trk_sce_end_x_v')\n",
    "    trk_sce_end_y_v = up.array('trk_sce_end_y_v')\n",
    "    trk_sce_end_z_v = up.array('trk_sce_end_z_v')\n",
    "    shr_id = up.array('shr_id')-1 # I think we need this -1 to get the right result\n",
    "    #trk_score_v_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else -9999. for pidv,tid in zip(trk_score_v,shr_id)])\n",
    "    trk_sce_start_x_v_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else -9999. for pidv,tid in zip(trk_sce_start_x_v,shr_id)])\n",
    "    trk_sce_start_y_v_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else -9999. for pidv,tid in zip(trk_sce_start_y_v,shr_id)])\n",
    "    trk_sce_start_z_v_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else -9999. for pidv,tid in zip(trk_sce_start_z_v,shr_id)])\n",
    "    trk_sce_end_x_v_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else -9999. for pidv,tid in zip(trk_sce_end_x_v,shr_id)])\n",
    "    trk_sce_end_y_v_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else -9999. for pidv,tid in zip(trk_sce_end_y_v,shr_id)])\n",
    "    trk_sce_end_z_v_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else -9999. for pidv,tid in zip(trk_sce_end_z_v,shr_id)])\n",
    "    #df['shr_score_check'] = trk_score_v_sel\n",
    "    df['shr_trk_sce_start_x'] = trk_sce_start_x_v_sel\n",
    "    df['shr_trk_sce_start_y'] = trk_sce_start_y_v_sel\n",
    "    df['shr_trk_sce_start_z'] = trk_sce_start_z_v_sel\n",
    "    df['shr_trk_sce_end_x'] = trk_sce_end_x_v_sel\n",
    "    df['shr_trk_sce_end_y'] = trk_sce_end_y_v_sel\n",
    "    df['shr_trk_sce_end_z'] = trk_sce_end_z_v_sel\n",
    "    # some more clean up\n",
    "    df.loc[df['shrmoliereavg'].isna(),'secondshower_Y_dot'] = 9999.\n",
    "    df.loc[df['secondshower_Y_dot'].isna(),'secondshower_Y_dot'] = 0.0 \n",
    "    # some more variables definition\n",
    "    df[\"slclnhits\"]   = up.array(\"pfnhits\").sum()\n",
    "    df[\"slclnunhits\"] = up.array(\"pfnunhits\").sum()\n",
    "    df['shr_tkfit_nhits_tot']       = (df['shr_tkfit_nhits_Y']+df['shr_tkfit_nhits_U']+df['shr_tkfit_nhits_V'])\n",
    "    df['shr_tkfit_nhi']             = (df['shr_tkfit_nhits_Y']*df['shr_tkfit_dedx_Y'] + df['shr_tkfit_nhits_U']*df['shr_tkfit_dedx_U'] + df['shr_tkfit_nhits_V']*df['shr_tkfit_dedx_V'])/df['shr_tkfit_nhits_tot']\n",
    "    df['shr_tkfit_2cm_nhits_tot']   = (df['shr_tkfit_2cm_nhits_Y']+df['shr_tkfit_2cm_nhits_U']+df['shr_tkfit_2cm_nhits_V'])\n",
    "    df['shr_tkfit_2cm_dedx_avg']    = (df['shr_tkfit_2cm_nhits_Y']*df['shr_tkfit_2cm_dedx_Y'] + df['shr_tkfit_2cm_nhits_U']*df['shr_tkfit_2cm_dedx_U'] + df['shr_tkfit_2cm_nhits_V']*df['shr_tkfit_2cm_dedx_V'])/df['shr_tkfit_2cm_nhits_tot']\n",
    "    df['shr_tkfit_gap10_nhits_tot'] = (df['shr_tkfit_gap10_nhits_Y']+df['shr_tkfit_gap10_nhits_U']+df['shr_tkfit_gap10_nhits_V'])\n",
    "    df['shr_tkfit_gap10_dedx_avg']  = (df['shr_tkfit_gap10_nhits_Y']*df['shr_tkfit_gap10_dedx_Y'] + df['shr_tkfit_gap10_nhits_U']*df['shr_tkfit_gap10_dedx_U'] + df['shr_tkfit_gap10_nhits_V']*df['shr_tkfit_gap10_dedx_V'])/df['shr_tkfit_gap10_nhits_tot']\n",
    "    df.loc[:,'shr_tkfit_dedx_max']  = df['shr_tkfit_dedx_Y']\n",
    "    df.loc[(df['shr_tkfit_nhits_U']>df['shr_tkfit_nhits_Y']),'shr_tkfit_dedx_max'] = df['shr_tkfit_dedx_U']\n",
    "    df.loc[(df['shr_tkfit_nhits_V']>df['shr_tkfit_nhits_Y']) & (df['shr_tkfit_nhits_V']>df['shr_tkfit_nhits_U']),'shr_tkfit_dedx_max'] = df['shr_tkfit_dedx_V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "######## RUN THIS CELL ONLY ONE TIME ONLY ###############\n",
    "#########################################################\n",
    "# The flash time for the NuMI data stream needs to be adjusted a little bit (there are hardware offsets in data). \n",
    "# That's the reason behind these hard coded shifts\n",
    "df_v = [mc,nue,dirt]\n",
    "for i,df in enumerate(df_v):\n",
    "        df.loc[ :, 'flash_time' ] = df['flash_time' ] - 0.304\n",
    "\n",
    "df_v = [ext]\n",
    "for i,df in enumerate(df_v):\n",
    "        df.loc[ :, 'flash_time' ] = df['flash_time' ] - 0.359\n",
    "\n",
    "#########################################################\n",
    "######## RUN THIS CELL ONLY ONE TIME ONLY ###############\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We are defining a very important variable here: the neutrino reconstructed energy\n",
    "# With the next values, we are also making a tiny correction to the electron shower energies\n",
    "INTERCEPT = 0.0\n",
    "SLOPE = 0.83\n",
    "\n",
    "df_v = [mc,nue,ext,data,dirt]\n",
    "# define some energy-related variables\n",
    "for i,df in enumerate(df_v):\n",
    "    df[\"reco_e\"]     = (df[\"shr_energy_tot_cali\"] + INTERCEPT) / SLOPE + df[\"trk_energy_tot\"]\n",
    "    df[\"reco_e_qe\"]  = 0.938*((df[\"shr_energy\"]+INTERCEPT)/SLOPE)/(0.938 - ((df[\"shr_energy\"]+INTERCEPT)/SLOPE)*(1-np.cos(df[\"shr_theta\"])))\n",
    "    df[\"reco_e_rqe\"] = df[\"reco_e_qe\"]/df[\"reco_e\"]\n",
    "\n",
    "# and a way to filter out data\n",
    "for i,df in enumerate(df_v):\n",
    "    df[\"bnbdata\"] = np.zeros_like(df[\"shr_energy\"])\n",
    "    df[\"extdata\"] = np.zeros_like(df[\"shr_energy\"])\n",
    "data[\"bnbdata\"] = np.ones_like(data[\"shr_energy\"])\n",
    "ext[\"extdata\"]  = np.ones_like(ext[\"shr_energy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add back the cosmic category, for background only\n",
    "df_v = [mc,nue,ext,data,dirt]\n",
    "for i,df in enumerate(df_v):\n",
    "    df.loc[(df['category']!=1)&(df['category']!=10)&(df['category']!=11)&(df['category']!=111)&(df['slnunhits']/df['slnhits']<0.2), 'category'] = 4\n",
    "    \n",
    "# Define signal cathegory\n",
    "data[\"is_signal\"] = data[\"category\"] == 11\n",
    "nue [\"is_signal\"] = nue[\"category\"]  == 11\n",
    "mc  [\"is_signal\"] = mc[\"category\"]   == 11\n",
    "dirt[\"is_signal\"] = dirt[\"category\"] == 11\n",
    "ext [\"is_signal\"] = ext[\"category\"]  == 11\n",
    "\n",
    "# Handling the nuebar in our interaction categorization\n",
    "df_v = [mc,nue]\n",
    "for i,df in enumerate(df_v):\n",
    "    df.loc[ (df['category']== 11) & (df['nu_pdg'] == -12), 'category' ] = 12\n",
    "    df.loc[ (df['category']== 10) & (df['nu_pdg'] == -12), 'category' ] = 1\n",
    "    df.loc[ (df['category']== 10) & (df['nu_pdg'] ==  12), 'category' ] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XcI43ileYJ9P"
   },
   "outputs": [],
   "source": [
    "# This is our fancy BDT cell. If we want to cut on the trained BDT score, we need to import that \n",
    "# variable in the data frame!\n",
    "# The training is done somewhere else, but the paramters of the training are stored in the pickle file \n",
    "# (which you shoul have in the pickle folder). We just want to use the output of an example BDT  in our example selection (more on this later)\n",
    "\n",
    "# variables to be trained on\n",
    "TRAINVAR = [\"shr_score\",\"tksh_distance\",\"tksh_angle\",\n",
    "            \"shr_tkfit_dedx_max\",\"trkfit\",\"trkpid\",\n",
    "            \"subcluster\",\"shrmoliereavg\",\n",
    "            \"trkshrhitdist2\",\"hits_ratio\",\n",
    "            \"secondshower_Y_nhit\",\"secondshower_Y_vtxdist\",\"secondshower_Y_dot\",\"anglediff_Y\",\n",
    "            \"CosmicIPAll3D\",\"CosmicDirAll3D\"]\n",
    "\n",
    "LABELS = ['pi0','nonpi0']\n",
    "\n",
    "if (USEBDT == True):\n",
    "    for label, bkg_query in zip(LABELS, nue_booster.bkg_queries):\n",
    "        with open(ls.pickle_path+'booster_%s_0304_extnumi.pickle' % label, 'rb') as booster_file:\n",
    "            booster = pickle.load(booster_file)\n",
    "            mc[label+\"_score\"] = booster.predict(\n",
    "                xgb.DMatrix(mc[TRAINVAR]),\n",
    "                ntree_limit=booster.best_iteration)\n",
    "            nue[label+\"_score\"] = booster.predict(\n",
    "                xgb.DMatrix(nue[TRAINVAR]),\n",
    "                ntree_limit=booster.best_iteration)\n",
    "            ext[label+\"_score\"] = booster.predict(\n",
    "                xgb.DMatrix(ext[TRAINVAR]),\n",
    "                ntree_limit=booster.best_iteration)\n",
    "            data[label+\"_score\"] = booster.predict(\n",
    "                xgb.DMatrix(data[TRAINVAR]),\n",
    "                ntree_limit=booster.best_iteration)\n",
    "            dirt[label+\"_score\"] = booster.predict(\n",
    "                xgb.DMatrix(dirt[TRAINVAR]),\n",
    "                ntree_limit=booster.best_iteration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing a simple selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You write a selection as a long string with and/or statements\n",
    "# You can concatenate simply these statements, or define make more than one test selection (it's just another string!)\n",
    "# We have a few examples here to choose from when plotting....\n",
    "\n",
    "# nue preselection\n",
    "PRESQ = 'nslice == 1'\n",
    "PRESQ += ' and selected == 1'\n",
    "PRESQ += ' and shr_energy_tot_cali > 0.07'\n",
    "PRESQ += ' and ( (_opfilter_pe_beam > 0 and _opfilter_pe_veto < 20) or bnbdata == 1 or extdata == 1)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1eNp preselection\n",
    "NPPRESQ = PRESQ\n",
    "NPPRESQ += ' and n_tracks_contained > 0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very loose box cuts\n",
    "NPVLCUTQ = NPPRESQ\n",
    "NPVLCUTQ += ' and CosmicIPAll3D > 10.'\n",
    "NPVLCUTQ += ' and trkpid < 0.25'\n",
    "NPVLCUTQ += ' and hits_ratio > 0.5'\n",
    "NPVLCUTQ += ' and trkfit < 0.90'\n",
    "NPVLCUTQ += ' and n_showers_contained == 1'\n",
    "NPVLCUTQ += ' and tksh_distance < 10.0'\n",
    "NPVLCUTQ += ' and tksh_angle > -0.9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BDT cuts\n",
    "# the \"pi0_score\" and \"nonpi0_score\" are the outcome of the BDT training that we defined in the \"fancy BDT cell\" do you remember?\n",
    "#BDTCQ = NPPRESQ\n",
    "BDTCQ = 'pi0_score > 0.5'# and nonpi0_score < 0.50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection that tries to isolate 2 protons\n",
    "NN = NPVLCUTQ \n",
    "NN += ' and N_recoProtons == 2 ' \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  POT normalization & Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plotter.py is where most of the plotting magic happens (go check that class out)\n",
    "# We need to import it here.\n",
    "import plotter\n",
    "import importlib\n",
    "importlib.reload(plotter)\n",
    "\n",
    "# In order to make a data-MC comparison plot, we need to define the samples that we feed the plotter\n",
    "# as well as the relative POT normalization for each sample: \n",
    "# we need to scale the statistics of the various samples to be comparable the data!\n",
    "\n",
    "# Let's define the samples\n",
    "nue   = nue.query(\"(abs(nu_pdg) == 12 & ccnc == 0 & true_nu_vtx_z < 1036.8 & true_nu_vtx_z > 0 & true_nu_vtx_y < 116.5 & true_nu_vtx_y > -116.5 & true_nu_vtx_x < \\\n",
    " 254.8 & true_nu_vtx_x > -1.55)\")\n",
    "samples = {\n",
    "    \"mc\": mc,\n",
    "    \"nue\": nue,\n",
    "    \"data\": data,\n",
    "    \"ext\": ext,\n",
    "    \"dirt\": dirt\n",
    "}\n",
    "\n",
    "\n",
    "# There's a few different ways you can figure out the number for your scaling. \n",
    "# I am putting here the numbers needed for the most up to date samples as of Dec 23rd 2020\n",
    "# One of the easiest is to use the NuMI_POT_counting notebook in this package (take a look at how that works!)\n",
    "\n",
    "# This is the data POT we normalize to (comes from the data Ntuples)\n",
    "pot =  2e+20 \n",
    "# This is the full MC POT scaling factor\n",
    "mcratio   = pot/2.32135e+21 \n",
    "# This is the POT scaling factor for the nue CC only sample \n",
    "nueratio  = pot/2.59311e+22\n",
    "# This is the POT scaling factor for dirt sample\n",
    "dirtratio = pot/1.42143e+21\n",
    "# The EXT sample (off beam) is scaled using the triggers (there's not POT in the external sample!)\n",
    "extratio  = 5268051/9199232.74 \n",
    "# a little bit of a scaling tweak\n",
    "scalingEXT = 0.98 # We scale down the EXT to account for neutrino occupancy\n",
    "scalingDRT = 0.35 # We scale down the dirt (big uncertainties on this, there's a better data-MC agreement like this, but this needs checking)\n",
    "\n",
    "# Alright, now we can define the sample normalization factors\n",
    "normalization = {\"mc\"  : mcratio  ,   \n",
    "                 \"nue\" : nueratio ,  \n",
    "                 \"ext\" : extratio  * scalingEXT, \n",
    "                 \"dirt\": dirtratio * scalingDRT}\n",
    "\n",
    "my_plotter = plotter.Plotter(samples, normalization, pot=pot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2342,
     "status": "ok",
     "timestamp": 1560557343774,
     "user": {
      "displayName": "Stefano Roberto Soleti",
      "photoUrl": "https://lh4.googleusercontent.com/-hfLpspJu4Q0/AAAAAAAAAAI/AAAAAAAABmA/2kE4rtj8paU/s64/photo.jpg",
      "userId": "10372352518008961760"
     },
     "user_tz": 240
    },
    "id": "b93hN-pGYJ9T",
    "outputId": "17e7c7ed-3f12-4b03-805c-6698f1617878",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ready for the big moment? \n",
    "# We're plotting!!!\n",
    "\n",
    "# We have many selections to choose from, let's pick one\n",
    "QUERY = BDTCQ \n",
    "# Let's decide what variable to plot (and the bin size)\n",
    "VARIABLE, BINS, RANGE, XTIT = 'reco_e', 10, (0.15,2.15), r\"Reconstructed Energy [GeV]\"\n",
    "\n",
    "# For all the plotting options look at plotting.py\n",
    "fig, ax1, ax2 = my_plotter.plot_variable(\n",
    "    VARIABLE,   \n",
    "    query= QUERY ,\n",
    "    kind=\"event_category\", # this splits up the MC in many physics categories\n",
    "    draw_sys  = True, # We plot the PPFX, GENIE and ReInteraction Systematics (the other systematics are to be implemented), NOTE:there's a tiny issue with the dirt sample that needs a workaround\n",
    "    draw_geoSys = True,\n",
    "    #draw_data = False, # we can also plot the MC only\n",
    "    genieweight = \"weightSplineTimesTuneTimesPPFX\", # This is the official weight we calculated before. Let's use it!\n",
    "    stacksort=3, # oder of categories\n",
    "    title=XTIT,  # Name of X axis\n",
    "    bins=BINS,\n",
    "    range=RANGE,\n",
    ")[0:3]\n",
    "\n",
    "\n",
    "ax1.set_title('NuMI Run 1, Your Cool Selection -- Partial Sys',loc='left')\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"/Users/elenag/Desktop/NuMI/NuMI4PeLEE/Sidebands/Figures/NuMI/1eNp/BDTFinalFullMC/\"+VARIABLE+\"_S_Intrinsic_ReintFluxXsPPFX.pdf\")\n",
    "#print('%s P-value Full Cov = %.4f Stat Only = %.4f Diag Only = %.4f ' % (VARIABLE, my_plotter.stats['pvalue'], my_plotter.stats['pvaluestatonly'],my_plotter.stats['pvaluediag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate data-MC agrement accordinf to p-value (definition in plotter.py)\n",
    "print('%s P-value Full Cov = %.4f Stat Only = %.4f Diag Only = %.4f ' % (VARIABLE, my_plotter.stats['pvalue'], my_plotter.stats['pvaluestatonly'],my_plotter.stats['pvaluediag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wanna do a list dump of your events?\n",
    "print(data.query(QUERY)[[\"run\",\"sub\",\"evt\"]])\n",
    "# and maybe create a small samweb definition that isolates those (so you can look at the in the event display?)\n",
    "f = data.query(QUERY)[[\"run\",\"sub\",]]\n",
    "query4SamWeb = f.apply(lambda x: str(x['run'])+'.'+str(x['sub'])+',', axis=1).sum()\n",
    "# print what the query for sameweb would look like:\n",
    "print(\"defname: numi_uboone_run1_beamon_offset1_mcc9_reco2_v08_00_00_28_beam_good and run_number \"+query4SamWeb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's do a plot dump: define a loads of plots you want to take a look at and go bananas.\n",
    "plots = []\n",
    "\n",
    "# Just a few as as examples. \n",
    "# PLEASE: if you're doing a BDT optimization, you better check the data-MC agreement of your input variables BEFORE\n",
    "# you select on them (and before you train the BDT using them)\n",
    "plots.append(['reco_nu_vtx_sce_x',10,(0,260),\"vtx x [cm]\"])\n",
    "plots.append(['reco_nu_vtx_sce_y',10,(-120,120),\"vtx y [cm]\"])\n",
    "plots.append(['reco_nu_vtx_sce_z',10,(0,1030),\"vtx z [cm]\"])\n",
    "\n",
    "if USEBDT:\n",
    "    plots.append(['pi0_score',10,(0,1.0),\"$\\pi^0$ BDT response\"])\n",
    "    plots.append(['nonpi0_score',10,(0,1.0),\"non-$\\pi^0$ BDT response\"])\n",
    "\n",
    "# I'm also saving the p-values in a text file for convenience\n",
    "f = open(\"/Users/elenag/Desktop/NuMI/NuMI4PeLEE/Sidebands/Figures/NuMI/1eNp/BDTFinalFullMC/NuMIPValuesSys_1eNp.txt\", \"w\")\n",
    "for VARIABLE, BINS, RANGE, XTIT in plots:\n",
    "    print(VARIABLE, BINS, RANGE, XTIT)\n",
    "    fig, ax1, ax2 = my_plotter.plot_variable(\n",
    "        VARIABLE,   \n",
    "        query=QUERY,\n",
    "        kind=\"event_category\",\n",
    "        genieweight = \"weightSplineTimesTuneTimesPPFX\",\n",
    "        draw_sys=True,\n",
    "        stacksort=3,\n",
    "        title=XTIT,\n",
    "        bins=BINS,\n",
    "        range=RANGE,\n",
    "    )[0:3]\n",
    "    ax1.set_title('NuMI Run 1, 1eNp Final Sel -- HP,XSec,G4 Syst',loc='left')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"/Users/elenag/Desktop/\"+VARIABLE+\"_S_FullMC_ReintFluxXsPPFX.pdf\")\n",
    "    f.write('%s %.4f %.4f %.4f ' % (VARIABLE, my_plotter.stats['pvalue'], my_plotter.stats['pvaluestatonly'],my_plotter.stats['pvaluediag'])+\"\\n\")\n",
    "    #break\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Plotter.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
